# -*- coding: utf-8 -*-
"""machinelearning_models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-lnb0cJoDEKjREQJLNH55Ely83AZuLr8

# Import Libraries
"""

import io
import os
import warnings
import pandas as pd
import seaborn as sns
from google.colab import files
import matplotlib.pyplot as plt

warnings.filterwarnings(action='ignore')

from google.colab import drive
drive.mount('/content/drive')

#df_test=pd.read_csv('/content/drive/MyDrive/diet_allowed.csv', encoding='ISO-8859–1') #preprocessed dataset
df_test=pd.read_csv('/content/drive/MyDrive/food_products_dataset.csv', encoding='ISO-8859–1') #original dataset

df_test['Label (Diet Not Allowed)'] = df_test["Label (Diet Allowed)"].replace({"vegan" : 1,"lacto" : 2,"None" : 3,"ovo" : 4,"Pesco" : 5,"Pollo" : 6,"lactoovo" : 7})

"""1. Run all algorithm of lazyclassifier
 2. Choose the best performing and time efficient algorithms (from different ML families)
 3. Optimize these algorithms
 4. Report results of the optimized algorithm using cross validation 10 times
 5. Export results to excel and add it to the report
 6. Choose one algorithm
 7. Calculate Standard deviation (metrics score +- standard deviation)

# Import and Installation for machine learning model
"""

pip install lazypredict

pip install scikit-optimize

import torch
import lazypredict
import numpy as np
import tensorflow as tf
from skopt                   import BayesSearchCV
from torch.utils.data        import DataLoader, TensorDataset
from sklearn.metrics         import accuracy_score
from sklearn.metrics         import confusion_matrix
from sklearn.neural_network  import MLPClassifier
from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold
from lazypredict.Supervised  import LazyClassifier
from sklearn.preprocessing   import StandardScaler
from sklearn.naive_bayes     import BernoulliNB
from sklearn.model_selection import RandomizedSearchCV

from sklearn                 import metrics
from sklearn.metrics         import recall_score
from sklearn.metrics         import f1_score
from sklearn.metrics         import precision_score

"""# Data Preparation

Remove Null Values
"""

import re

df['features.value'].fillna('No Info', inplace = True)
df['name'].fillna('No Info', inplace = True)

"""Remove non-ASCII characters"""

def remove_non_ascii_2(text):
    return ''.join([i if ord(i) < 128 else ' ' for i in text])

"""Remove unnecessary words"""

def remove_unnecessary_words(text):
  regex = re.compile('[^a-zA-Z%,0-9-]')
  #First parameter is the replacement, second parameter is your input string
  sentence = regex.sub(' ', text)

  testReplace = ['and', 'And', 'Contains', 'in','a','that']

  splitSentence = sentence.split(" ")

  #for word in splitSentence:
  for i in range(len(splitSentence)):
    for removedWord in testReplace:
      if(splitSentence[i] == removedWord):
        splitSentence[i] = ","
    
  sentence = " ".join(splitSentence)

  #Replace multiple space with single space
  sentence = " ".join(sentence.split())
  
  return sentence
  #print(sentence)

remove_unnecessary_words()

"""Label for food categories in training dataset"""

chicken = df.loc[df['features.value'].str.contains("chicken", case=False)]
beef    = df.loc[df['features.value'].str.contains("beef", case=False)]
pork    = df.loc[df['features.value'].str.contains("bacon", case=False)]
dairy   = df.loc[df['features.value'].str.contains("milk", case=False)]
egg     = df.loc[df['features.value'].str.contains("egg", case=False)]
grain   = df.loc[df['features.value'].str.contains("grain", case=False)]
wheat   = df.loc[df['features.value'].str.contains("wheat", case=False)]
fish    = df.loc[df['features.value'].str.contains("fish", case=False)]
crab    = df.loc[df['features.value'].str.contains("crab", case=False)]

"""Change value of all categories to 1 if true in dataset"""

for idx, x in df['features.value'].iteritems():
  for idx2, y in chicken['features.value'].iteritems():
    if (idx == idx2):
      df['Chicken'].loc[idx2] = 1

for idx, x in df['features.value'].iteritems():
  for idx2, y in beef['features.value'].iteritems():
    if (idx == idx2):
      df['Beef'].loc[idx2] = 1

for idx, x in df['features.value'].iteritems():
  for idx2, y in pork['features.value'].iteritems():
    if (idx == idx2):
      df['Pork'].loc[idx2] = 1

for idx, x in df['features.value'].iteritems():
  for idx2, y in dairy['features.value'].iteritems():
    if (idx == idx2):
      df['Dairy'].loc[idx2] = 1

for idx, x in df['features.value'].iteritems():
  for idx2, y in egg['features.value'].iteritems():
    if (idx == idx2):
      df['Egg'].loc[idx2] = 1

for idx, x in df['features.value'].iteritems():
  for idx2, y in grain['features.value'].iteritems():
    if (idx == idx2):
      df['Grain'].loc[idx2] = 1
  for idx2, y in wheat['features.value'].iteritems():
    if (idx == idx2):
      df['Grain'].loc[idx2] = 1

for idx, x in df['features.value'].iteritems():
  for idx2, y in seafood['features.value'].iteritems():
    if (idx == idx2):
      df['Seafood'].loc[idx2] = 1

"""Autolabelling for dataset"""

for i in range(len(df['features.value'])):
  for index, y in chicken['features.value'].iteritems():
    if (i == index):
      df['Chicken'].loc[index] = 1

for i in range(len(df['features.value'])):
  for index, y in beef['features.value'].iteritems():
    if (i == index):
      df['Beef'].loc[index] = 1

for i in range(len(df['features.value'])):
  for index, y in pork['features.value'].iteritems():
    if (i == index):
      df['Pork'].loc[index] = 1    

for i in range(len(df['features.value'])):
  for index, y in dairy['features.value'].iteritems():
    if (i == index):
      df['Dairy'].loc[index] = 1

for i in range(len(df['features.value'])):
  for index, y in egg['features.value'].iteritems():
    if (i == index):
      df['Egg'].loc[index] = 1

for i in range(len(df['features.value'])):
  for index, y in grain['features.value'].iteritems():
    if (i == index):
      df['Grain'].loc[index] = 1
  for index, y in wheat['features.value'].iteritems():
    if (i == index):
      df['Grain'].loc[index] = 1

for i in range(len(df['features.value'])):
  for index, y in fish['features.value'].iteritems():
    if (i == index):
      df['Seafood'].loc[index] = 1

for i in range(len(df['features.value'])):
  for index, y in crab['features.value'].iteritems():
    if (i == index):
      df['Seafood'].loc[index] = 1

"""# Data Preprocess"""

df_test['Label (Diet Allowed)'] = df_test["Label (Diet Allowed)"].replace({"vegan" : 1,"lacto" : 2,"None" : 3,"ovo" : 4,"Pesco" : 5,"Pollo" : 6,"lactoovo" : 7})

x = df_test.drop(["Label (Diet Allowed)"], axis = 1)
y = df_test["Label (Diet Allowed)"]
label = pd.get_dummies(df_test, columns = ['Label (Diet Allowed)'])
train, test = train_test_split(df_test, test_size = 0.2, random_state = 55)
x_train = train.drop(["Label (Diet Allowed)"], axis = 1)
x_test  = test.drop(["Label (Diet Allowed)"], axis = 1)
y_train = train["Label (Diet Allowed)"]
y_test  = test["Label (Diet Allowed)"]
x_train = torch.tensor(x_train.to_numpy(), dtype=torch.float32)
x_test  = torch.tensor(x_test.to_numpy(), dtype=torch.float32)
y_train = torch.tensor(y_train.to_numpy(), dtype=torch.float32)
y_test  = torch.tensor(y_test.to_numpy(), dtype=torch.float32)
train= TensorDataset(x_train, y_train)
test = TensorDataset(x_test, y_test)
train_dataloader = DataLoader(train, batch_size = 64, shuffle=True)
test_dataloader = DataLoader(test, batch_size = 64, shuffle=False)

linearSVC_fscore = []
perceptron_fscore = []
xgb_fscore = []
CalibratedCV_fscore = []
svc_fscore = []
sgd_fscore = []
nuSVC_fscore = []
logistic_reg_fscore = []
extraTrees_fscore = []
gaussianNB_fscore = []
kNeighbours_fscore = []
labelPropagation_fscore = []
lgbm_fscore = []
bagging_fscore = []

linearSVC_fscore = []
perceptron_fscore = []
xgb_fscore = []
CalibratedCV_fscore = []
svc_fscore = []
sgd_fscore = []
nuSVC_fscore = []
logistic_reg_fscore = []
extraTrees_fscore = []
gaussianNB_fscore = []
kNeighbours_fscore = []
labelPropagation_fscore = []
lgbm_fscore = []
bagging_fscore = []

"""Function for calculating average accuracy and f-score"""

def calculateMetrics(model_results):
  score = 0
  avg_score = 0
  for i in range(0,50):
    score += model_results[i]
  avg_score = acc_score/50;
  return(avg_score)

"""# Evaluation of dataset on LazyClassifier"""

#Calculates Accuracy from Confusion Matrix
def accuracy(confusion_matrix):
   diagonal_sum = confusion_matrix.trace()
   print (diagonal_sum)
   sum_of_all_elements = confusion_matrix.sum()
   print(sum_of_all_elements)
   return diagonal_sum / sum_of_all_elements

for random_state in range(1,11): 
  kf = KFold(n_splits=5, shuffle = True, random_state = random_state)
  for train_index , test_index in kf.split(x):
      x_train , x_test = x.iloc[train_index,:],x.iloc[test_index,:]
      y_train , y_test = y[train_index] , y[test_index]
      
      clf = LazyClassifier(verbose=0,ignore_warnings=True)
      models, predictions = clf.fit(x_train, x_test, y_train, y_test)
      model_result = models

      linearSVC.append(model_result['Accuracy']['LinearSVC'])
      perceptron.append(model_result['Accuracy']['Perceptron'])
      xgb.append(model_result['Accuracy']['XGBClassifier'])
      CalibratedCV.append(model_result['Accuracy']['CalibratedClassifierCV'])
      svc.append(model_result['Accuracy']['SVC'])
      sgd.append(model_result['Accuracy']['SGDClassifier'])
      nuSVC.append(model_result['Accuracy']['NuSVC'])
      logistic_reg.append(model_result['Accuracy']['LogisticRegression'])
      extraTrees.append(model_result['Accuracy']['ExtraTreesClassifier'])
      gaussianNB.append(model_result['Accuracy']['GaussianNB'])
      kNeighbours.append(model_result['Accuracy']['KNeighborsClassifier'])
      labelPropagation.append(model_result['Accuracy']['LabelPropagation'])
      lgbm.append(model_result['Accuracy']['LGBMClassifier'])
      bagging.append(model_result['Accuracy']['BaggingClassifier'])
      
      linearSVC_fscore.append(model_result['F1 Score']['LinearSVC'])
      perceptron_fscore.append(model_result['F1 Score']['Perceptron'])
      xgb_fscore.append(model_result['F1 Score']['XGBClassifier'])
      CalibratedCV_fscore.append(model_result['F1 Score']['CalibratedClassifierCV'])
      svc_fscore.append(model_result['F1 Score']['SVC'])
      sgd_fscore.append(model_result['F1 Score']['SGDClassifier'])
      nuSVC_fscore.append(model_result['F1 Score']['NuSVC'])
      logistic_reg_fscore.append(model_result['F1 Score']['LogisticRegression'])
      extraTrees_fscore.append(model_result['F1 Score']['ExtraTreesClassifier'])
      gaussianNB_fscore.append(model_result['F1 Score']['GaussianNB'])
      kNeighbours_fscore.append(model_result['F1 Score']['KNeighborsClassifier'])
      labelPropagation_fscore.append(model_result['F1 Score']['LabelPropagation'])
      lgbm_fscore.append(model_result['F1 Score']['LGBMClassifier'])
      bagging_fscore.append(model_result['F1 Score']['BaggingClassifier'])
  print("Done with random_state : ", random_state)

"""Standard Deviation of Accuracy"""

print(np.std(linearSVC)*100)
print(np.std(perceptron)*100)
print(np.std(xgb)*100)
print(np.std(CalibratedCV)*100)
print(np.std(svc)*100)
print(np.std(sgd)*100)
print(np.std(nuSVC)*100)
print(np.std(logistic_reg)*100)
print(np.std(extraTrees)*100)
print(np.std(gaussianNB)*100)
print(np.std(kNeighbours)*100)
print(np.std(labelPropagation)*100)
print(np.std(lgbm)*100)
print(np.std(bagging)*100)

"""Standard Deviation of F-Score"""

print(np.std(linearSVC_fscore)*100)
print(np.std(perceptron_fscore)*100)
print(np.std(xgb_fscore)*100)
print(np.std(CalibratedCV_fscore)*100)
print(np.std(svc_fscore)*100)
print(np.std(sgd_fscore)*100)
print(np.std(nuSVC_fscore)*100)
print(np.std(logistic_reg_fscore)*100)
print(np.std(extraTrees_fscore)*100)
print(np.std(gaussianNB_fscore)*100)
print(np.std(kNeighbours_fscore)*100)
print(np.std(labelPropagation_fscore)*100)
print(np.std(lgbm_fscore)*100)
print(np.std(bagging_fscore)*100)

"""#Optimization of selected 3 models

Multilayer Perceptron
"""

params = dict()
params['hidden_layer_sizes'] = [150, 50, 100]
params['activation'] = ['identity', 'logistic', 'tanh', 'relu']
params['solver'] = ['lbfgs', 'sgd', 'adam']
params['alpha'] = [0.0001]
#params['batch_size'] = [16, 32, 64, 128]
#params['learning_rate'] = ('constant', 'invscaling', 'adaptive')         #used only for sgd
#params['learning_rate_init'] = ('identity', 'logistic', 'tanh', 'relu')  #used only for sgd and adam
#params['power_t'] = ('identity', 'logistic', 'tanh', 'relu')
#params['shuffle'] = (True, False)
#params['random_state'] = ('none')                                        #used only for sgd and adam
#params['tol'] = ('identity', 'logistic', 'tanh', 'relu')                 #used when learning_rate is adaptive
#params['verbose'] = ('identity', 'logistic', 'tanh', 'relu')             #used to print progress
#params['warm_start'] = (True, False)
#params['max_iter'] = (100, 200, 500, 1000)
#params['momentum'] = ('identity', 'logistic', 'tanh', 'relu')            #used only for sgd
#params['nesterovs_momentum'] = ('identity', 'logistic', 'tanh', 'relu')  #used only for sgd and momentum > 0
#params['early_stopping'] = ('identity', 'logistic', 'tanh', 'relu')      #used only for adam
#params['beta_1'] = ('identity', 'logistic', 'tanh', 'relu')              #used only for adam
#params['beta_2'] = ('identity', 'logistic', 'tanh', 'relu')              #used only for adam
#params['epsilon'] = ('identity', 'logistic', 'tanh', 'relu')             #used only for adam
#params['n_iter_no_change'] = ('identity', 'logistic', 'tanh', 'relu')    #used only for sgd and adam
#params['max_fun'] = ('identity', 'logistic', 'tanh', 'relu')             #used only for lbfgs

# define the search
search = BayesSearchCV(estimator=MLPClassifier(), search_spaces=params, cv=kf)

# perform the search
search.fit(x_train, y_train)

# report the best result
print(search.best_score_)
print(search.best_params_)

"""Bernoulli Naive Bayes"""

params = { 'alpha': [0.0, 0.2, 0.4, 0.6, 0.8, 1.0],
           'binarize': [0.0, 0.2, 0.4, None],
           'fit_prior': [True],
           'class_prior': [None]}
clf = RandomizedSearchCV(estimator=BernoulliNB(),
                         param_distributions=params,
                         scoring='neg_mean_squared_error',
                         n_iter=25,
                         verbose=1)
clf.fit(x_train, y_train)
print("Best parameters:", clf.best_params_)
print("Lowest RMSE: ", (-clf.best_score_)**(1/2.0))

"""# Training Optimized Model

Multilayer Perceptron
"""

for random_state in range(1,11): 
  kf = KFold(n_splits=5, shuffle = True, random_state = random_state)
  for train_index , test_index in kf.split(x):
    x_train , x_test = x.iloc[train_index,:],x.iloc[test_index,:]
    y_train , y_test = y[train_index] , y[test_index]
    
    mlp_clf = MLPClassifier(hidden_layer_sizes=50, max_iter=200,activation = 'logistic',solver='adam',alpha = 0.0001)
    mlp_model = mlp_clf.fit(x_train, y_train)

    y_pred    = mlp_model.predict(x_test)
    cm        = confusion_matrix(y_pred, y_test)

    #Printing the accuracy
    print(accuracy(cm))

    #acc_score = accuracy_score(y_test, y_pred)
    #fscore = metrics.f1_score(y_test, y_pred, average='macro')
    #recall = metrics.recall_score(y_test, y_pred, average='macro')
    #precision = precision_score(y_test, y_pred, average='macro')
    #print(fscore)
    #print(recall)
    #print(precision)

"""Bernoulli Naive Bayes"""

for random_state in range(1,11): 
  kf = KFold(n_splits=5, shuffle = True, random_state = random_state)
  for train_index , test_index in kf.split(x):
    x_train , x_test = x.iloc[train_index,:],x.iloc[test_index,:]
    y_train , y_test = y[train_index] , y[test_index]
    
    bernoulli_clf = BernoulliNB(alpha = 0.6, binarize = None, fit_prior = True, class_prior = None)
    bernoulli_clf.fit(x_train, y_train)

    y_pred    = bernoulli_clf.predict(x_test)
    cm        = confusion_matrix(y_pred, y_test)

    #Printing the accuracy
    print(accuracy(cm))

    #fscore = metrics.f1_score(y_test, y_pred, average='macro')
    #print(fscore)
    #recall = metrics.recall_score(y_test, y_pred, average='macro')
    #print(recall)
    #precision = precision_score(y_test, y_pred, average='macro')
    #print(precision)

"""# Export Model"""

import pickle
filename = 'finalized_model.sav'
pickle.dump(mlp_model, open(filename, 'wb'))

# some time later...
 
# load the model from disk
loaded_model = pickle.load(open(filename, 'rb'))
result = loaded_model.predict(x_test)
print(result)