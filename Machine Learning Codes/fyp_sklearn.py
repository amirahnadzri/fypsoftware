# -*- coding: utf-8 -*-
"""FYP_SKLearn

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tx_qYJnPnm6h-56l38CmymNw43LDIkVv

# Drive Mounting (Intro)
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import torch
from torch.utils.data import DataLoader
from google.colab import files
import io
import warnings
import os
import numpy as np

warnings.filterwarnings(action='ignore')

from google.colab import drive
drive.mount('/content/drive')

df=pd.read_csv('/content/drive/MyDrive/oversampled_data.csv', encoding='ISO-8859â€“1')

print(list(df))

"""# Data Preprossessing"""

#Rename Column
df.rename(columns = {'Label (Diet Not Allowed)':'Diet_Allowed'}, inplace = True)

df['Diet_Allowed'] = df["Diet_Allowed"].replace({"All" : 0,"lacto" : 1,"None" : 2,"ovo" : 3,"Pesco" : 4, "PescoPollo" : 5,"Pollo" : 6,"vegan" : 7}).astype(np.int64)

"""

# Data Visualization"""

# Create dataframe of full IMDb movies with ratings > 0.0
# Plot Graph:
# Num of titles in IMDb dataset by ratings > 0.0

fig, ax = plt.subplots(figsize=(30, 12))
sns.countplot(df.Diet_Allowed, palette="Set1").set(title='Label Destribution')

df['Diet_Allowed'].value_counts()

"""

# Model Training"""

import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.model_selection import GridSearchCV
import xgboost as xgb
from sklearn import metrics

from sklearn.model_selection import GridSearchCV, train_test_split

#Spliting data to 8:2
train = df
train, test = train_test_split(train, test_size = 0.2, random_state = 55)

train_x, train_y = train.drop(columns = ['Diet_Allowed']), train['Diet_Allowed']
test_x, test_y = test.drop(columns = ['Diet_Allowed']), test['Diet_Allowed']

#split dataset in features and target variable
feature_cols = ['Poultry', 'Beef', 'Pork', 'Dairy', 'Egg', 'Grain', 'Seafood']
X = df[feature_cols] # Features
y = df.Diet_Allowed # Target variable

train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=55)

#Creating a dictionary for result
result_dict = dict()

x, y = df.iloc[:,:-1],df.iloc[:,-1]
data_dmatrix = xgb.DMatrix(data=x,label=y)

xgbtree = xgb.XGBClassifier()
xgbtree = xgbtree.fit(train_X,train_y)
y_pred = xgbtree.predict(test_X)

# Optimized Version
xgbtree = xgb.XGBClassifier(subsample= 0.8999999999999999, n_estimators= 500, max_depth= 6, learning_rate= 0.2, colsample_bytree= 0.6, colsample_bylevel= 0.7999999999999999)
xgbtree = xgbtree.fit(train_X,train_y)
y_pred = xgbtree.predict(test_X)

print("Accuracy:",metrics.accuracy_score(test_y, y_pred))

train_acc = xgbtree.score(train_X, train_y)
test_acc = xgbtree.score(test_X, test_y)
print(f'Train Acc: {train_acc}\nTest Acc: {test_acc}')

"""#Finding Parameter"""

train = df

from sklearn.model_selection import KFold
dataset = np.array(train.index)
kf = KFold(n_splits=3, shuffle=True, random_state=42)
for train_idx, val_idx in kf.split(dataset):
  display(train.loc[dataset[train_idx]].head(3))

# The goal of skorch is to make it possible to use PyTorch with sklearn. 
# This is achieved by providing a wrapper around PyTorch that has an sklearn interface.
# https://skorch.readthedocs.io/en/stable/index.html
!pip install skorch

train

xgbtree = xgb.XGBClassifier()

from sklearn.model_selection import RandomizedSearchCV
kf = KFold(n_splits=5, shuffle = True, random_state = 1)
for train_index , test_index in kf.split(x):
    x_train , x_test = x.iloc[train_index,:],x.iloc[test_index,:]
    y_train , y_test = y[train_index] , y[test_index]
    params = { 'max_depth': [3, 5, 6, 10, 15, 20],
              'learning_rate': [0.01, 0.1, 0.2, 0.3],
              'subsample': np.arange(0.5, 1.0, 0.1),
              'colsample_bytree': np.arange(0.4, 1.0, 0.1),
              'colsample_bylevel': np.arange(0.4, 1.0, 0.1),
              'n_estimators': [100, 500, 1000]}
    clf = RandomizedSearchCV(estimator=xgbtree,
                            param_distributions=params, 
                            n_iter=25,
                            verbose=1)
    clf.fit(x_train, y_train)
    print("Best parameters:", clf.best_params_)
    print(f'Accuracy: {clf.best_score_ * 100 :.2f}%')

#split dataset in features and target variable
feature_cols = ['Poultry', 'Beef', 'Pork', 'Dairy', 'Egg', 'Grain', 'Seafood']
X = train[feature_cols] # Features
y = train.Diet_Allowed # Target variable

train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=55)

from sklearn.model_selection import RandomizedSearchCV
params = { 'max_depth': [3, 5, 6, 10, 15, 20],
           'learning_rate': [0.01, 0.1, 0.2, 0.3],
           'subsample': np.arange(0.5, 1.0, 0.1),
           'colsample_bytree': np.arange(0.4, 1.0, 0.1),
           'colsample_bylevel': np.arange(0.4, 1.0, 0.1),
           'n_estimators': [100, 500, 1000]}
clf = RandomizedSearchCV(estimator=xgbtree,
                         param_distributions=params,
                         scoring='neg_mean_squared_error',
                         n_iter=25,
                         verbose=1)
clf.fit(X, y)
print("Best parameters:", clf.best_params_)
print("Lowest RMSE: ", (-clf.best_score_)**(1/2.0))

clf.cv_results_

# 3. Apply into model using cross validated grid search method
from sklearn.model_selection import GridSearchCV
params = { 'max_depth': [6,7,8,9,10],
           'learning_rate': [0.2, 0.1, 0.01],
           'subsample': [0.5, 0.8999999999999999],
           'colsample_bytree': [0.6, 1.0],
           'colsample_bylevel': [0.4, 0.7999999999999999],
           'n_estimators': [100, 500, 1000]}
gs2 = GridSearchCV(xgbtree, 
                  params,
                  cv=3, 
                  verbose=0)

gs2.fit(X, y)
# 4. Return the best set of hyper-parameters for the model.
print(f'Accuracy: {gs2.best_score_ * 100 :.2f}%')
print(gs2.best_params_)

# 3. Apply into model using cross validated grid search method
from sklearn.model_selection import GridSearchCV
params = {'learning_rate':[0.01],'colsample_bytree': [0.3, 0.7], 'max_depth': list(range(3,5)),'n_estimators': [500,700],'subsample': [1,0.5,0.75] }
gs2 = GridSearchCV(xgbtree, 
                  params,
                  scoring=my_accuracy,
                  cv=3, 
                  verbose=0)

gs2.fit(x, y)
# 4. Return the best set of hyper-parameters for the model.
print(f'Accuracy: {gs2.best_score_ * 100 :.2f}%')
print(gs2.best_params_)

"""# Bayesian Optimiser"""

from sklearn.model_selection import KFold
dataset = np.array(train.index)
kf = KFold(n_splits=3, shuffle=True, random_state=42)
for train_idx, val_idx in kf.split(dataset):
  display(train.loc[dataset[train_idx]].head(3))

!pip install skorch

X = np.array(df.drop(['Diet_Allowed'], axis=1)).astype(np.int64)
y = np.array(df['Diet_Allowed']).astype(np.int64).reshape(-1, 1)

!pip install scikit_optimize

# example of bayesian optimization with scikit-optimize
from numpy import mean
from sklearn.model_selection import cross_val_score
from skopt.space import Integer
from skopt.space import Real
from skopt.utils import use_named_args
from skopt import gp_minimize
 

# define the model
model = xgb.XGBClassifier()

# define the space of hyperparameters to search
search_space = [Real(0.01, 0.1, name='learning_rate'),Real(0.5, 1, name='subsample'), Real(0.3, 0.7, name='colsample_bytree'), Integer(3, 5, name='max_depth'), Integer(500, 700, name='n_estimators') ]
 
# define the function used to evaluate a given configuration
@use_named_args(search_space)
def evaluate_model(**params):
	# something
	model.set_params(**params)
	# calculate 5-fold cross validation
	result = cross_val_score(model, X, y, cv=5, n_jobs=-1, scoring='accuracy')
	# calculate the mean of the scores
	estimate = mean(result)
	return 1.0 - estimate
 
# perform optimization
result = gp_minimize(evaluate_model, search_space)
# summarizing finding:
print('Best Accuracy: %.3f' % (1.0 - result.fun))
print('Best Parameters: n_neighbors=%d, p=%d' % (result.x[0], result.x[1]))

"""# Result Visualization

"""

#Correlation heat map

plt.figure(figsize = (10,5))
sns.heatmap(train.corr(), annot = True, cmap="rainbow")
plt.show()

# Comparison table


import numpy as np 
import matplotlib.pyplot as plt 
  
X = ['Decision Tree','XGBoost']
basic = [96.47,97.26]
parameter = [96.82, 97.26]
  
X_axis = np.arange(len(X))
  
plt.bar(X_axis - 0.2, basic, 0.4, label = 'Default')
plt.bar(X_axis + 0.2, parameter, 0.4, label = 'Best Parameter')
  
plt.xticks(X_axis, X)
plt.xlabel("Model")
plt.ylabel("Accuracy (%)")
plt.title("Comparison Table")
plt.legend()
plt.show()

def get_dt_graph(dt_classifier):
    fig = plt.figure(figsize=(25,20))
    _ = DecTree.plot_tree(dt_classifier,
                       feature_names=X.columns,
                       class_names=[1,2,3,4,5,6,7],
                       filled=True)

gph = get_dt_graph(DecTree)

"""# Average Accuracy"""

from sklearn.metrics         import recall_score
from sklearn.metrics         import f1_score
from sklearn.metrics         import precision_score
from sklearn.metrics         import accuracy_score
from sklearn.metrics         import confusion_matrix
from sklearn.model_selection import KFold
import numpy

def accuracy(confusion_matrix):
   diagonal_sum = confusion_matrix.trace()
   #print (diagonal_sum)
   sum_of_all_elements = confusion_matrix.sum()
   #print(sum_of_all_elements)
   return diagonal_sum / sum_of_all_elements

random_state=1

a = range(1,10)

x = df.drop(["Diet_Allowed"], axis = 1)
y = df["Diet_Allowed"]

for random_state in range(1,11):
  kf = KFold(n_splits=5, shuffle = True, random_state = random_state)
  for train_index , test_index in kf.split(x):
    x_train , x_test = x.iloc[train_index,:],x.iloc[test_index,:]
    y_train , y_test = y[train_index] , y[test_index]
    
    xgb_clf = xgb.XGBClassifier(subsample= 0.8999999999999999, n_estimators= 500, max_depth= 6, learning_rate= 0.2, colsample_bytree= 0.6, colsample_bylevel= 0.7999999999999999)
    xgb_model = xgb_clf.fit(x_train, y_train)

    y_pred    = xgb_model.predict(x_test)
    cm        = confusion_matrix(y_pred, y_test)

    #acc_score = accuracy(cm)
    #f_score = f1_score(y_test, y_pred, average='macro')
    rc_score = metrics.recall_score(y_test, y_pred, average='macro')
    #precis_score = metrics.precision_score(y_test, y_pred, average='macro')

    #Printing the accuracy
   # print("Accuracy of XGBoost : ", acc_score)
    #print("F-score of XGBoost : ", f_score)
    print( rc_score)
    #print("Precision of XGBoost : ",precis_score)

"""# Appendices

Dataframe Labelling
"""

for x, item in enumerate (df["Beef"]):
        # we check if there is beef or not
        # if there is and the Label (Diet Not Allowed) column is not null, the Label (Diet Not Allowed) would be none
        if item == 0 and pd.isnull(df['Label (Diet Not Allowed)'].loc[x]): 
            df['Label (Diet Not Allowed)'].loc[x] = "None"

for x, item in enumerate (df["Pork"]):
        # we check if there is pork or not
        # if there is and the Label (Diet Not Allowed) column is not null, the Label (Diet Not Allowed) would be none
        if item == 0 and pd.isnull(df['Label (Diet Not Allowed)'].loc[x]):
            df['Label (Diet Not Allowed)'].loc[x] = "None"

for x, item in enumerate (df["Label (Diet Not Allowed)"]):
        # we check if vegan can eat or not. 
        # if it checks the criteria and the Label (Diet Not Allowed) column is not null, the Label (Diet Not Allowed) would be Vegan
        if df["Poultry"].loc[x] == 0 and df["Seafood"].loc[x] == 0 and df["Egg"].loc[x] == 0 and df["Dairy"].loc[x] == 0:
           df["Label (Diet Not Allowed)"].loc[x] = "vegan"
        # we check if Lacto-Vegetarian can eat or not.
        # if it checks the criteria and the Label (Diet Not Allowed) column is not null, the Label (Diet Not Allowed) would be Lacto-Vegetarian
        elif df["Poultry"].loc[x] == 0 and df["Seafood"].loc[x] == 0 and df["Egg"].loc[x] == 0:
           df["Label (Diet Not Allowed)"].loc[x] = "lacto"
        # we check if Ovo-Vegetarian can eat or not.
        # if it checks the criteria and the Label (Diet Not Allowed) column is not null, the Label (Diet Not Allowed) would be Ovo-Vegetarian
        elif df["Poultry"].loc[x] == 0 and df["Seafood"].loc[x] == 0 and df["Dairy"].loc[x] == 0:
           df["Label (Diet Not Allowed)"].loc[x] = "ovo"
        # we check if Pesco can eat or not.
        # if it checks the criteria and the Label (Diet Not Allowed) column is not null, the Label (Diet Not Allowed) would be Pesco
        elif df["Egg"].loc[x] == 0 and df["Dairy"].loc[x] == 0 and df["Poultry"].loc[x] == 0:
           df["Label (Diet Not Allowed)"].loc[x] = "Pesco"
        # we check if Pollo can eat or not.
        # if it checks the criteria and the Label (Diet Not Allowed) column is not null, the Label (Diet Not Allowed) would be Pollo
        elif df["Egg"].loc[x] == 0 and df["Dairy"].loc[x] == 0 and df["Seafood"].loc[x] == 0:
           df["Label (Diet Not Allowed)"].loc[x] = "Pollo"
        # we check if PescoPollo can eat or not.
        # if it checks the criteria and the Label (Diet Not Allowed) column is not null, the Label (Diet Not Allowed) would be PescoPollo
        elif df["Egg"].loc[x] == 0 and df["Dairy"].loc[x] == 0:
           df["Label (Diet Not Allowed)"].loc[x] = "PescoPollo"
        elif pd.isnull(item):
           df["Label (Diet Not Allowed)"].loc[x] = "All"